---
title: "Homework5"
author: "Jeff Nguyen"
date: "26/11/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, envir=.GlobalEnv)
```

**Pre-MFE Probability Seminar**  
**Baruch College, Fall 2020**  


Homework 05  
Student Name: Ngoc Son (Jeff) Nguyen  


# **Question 1**  
  
Let $Y$ denotes the numeric representation of the coin toss. Assign $Y=1$ for $H$ and $Y=0$ for $T$.  
We're given: $P(\{H\}) = P(Y=1) = \frac{1}{5}$ and $P(\{T\}) = P(Y=0) = \frac{4}{5}$.  
  
Let further assign random variables $U$ and $Z$ as representations of $X$ based on the result of the coin toss as given: $U \sim U(0,1)$ and $Z \sim Exp(\lambda =1)$.  

We have:  


\begin{equation}
  \begin{aligned}
    X =& 1_{Y=1}*U + 1_{Y=0}*Z  \\
   \text{and so:}&  \\
    E(X) =& E\left( 1_{Y=1}*U + 1_{Y=0}*Z \right)  \\
         =& E\left( 1_{Y=1}*U \right) + E\left( 1_{Y=0}*Z \right)  \quad \text{(Summation property of expectation)} \\ 
  \end{aligned}
\end{equation}  

We note that the probability of the outcomes are independent of the outcome, i.e. $1_{Y=1}$ and $1_{Y=0}$ are independent of $U$ and $Z$. Thus, based on the product property of expectations:  

\begin{equation}
  \begin{aligned}
    E(X) =& E\left( 1_{Y=1}*U \right) + E\left( 1_{Y=0}*Z \right)  \\
         =& E\left( 1_{Y=1} \right) * E(U) + E\left( 1_{Y=0} \right) * E(Z)  \\
         =& P(Y=1) * E(U) + P(Y=0) * E(Z)  \\
         =& \frac{1}{5} * \frac{1+0}{2} + P(Y=0) * E(Z) \quad \text{(Property of uniform distribution)}  \\
         =& \frac{1}{5} * \frac{1}{2} + \frac{4}{5} * \frac{1}{1} \quad \text{(Property of exponential distribution)}  \\
         =& \frac{9}{10}  \\
  \end{aligned}
\end{equation}  

In addition:  

\begin{equation}
  \begin{aligned}
    Var(X) =& E(X^2) - E(X)^2  \\
           =& E\left( 1_{Y=1} \right) * E(U)^2 + E\left( 1_{Y=0} \right) * E(Z)^2 - E(X)^2  \\
           =& \frac{1}{5} * \left(\frac{1}{2}\right)^2 + \frac{4}{5} * 1^2 - \left(\frac{9}{10}\right)^2  \\
           =& \frac{1}{25}  \\
  \end{aligned}
\end{equation}  

# **Question 2**  

With $X \sim Exp(\lambda)$ and suppose $X \geqslant m$, we are trying to prove:  

\begin{equation}
  \begin{aligned}
    P( X \geqslant m + k | X \geqslant m) = P(X \geqslant k)
  \end{aligned}
\end{equation}  

In other words, we are proving that future probabilities $P( X \geqslant m + k | X \geqslant m)$ is independent on the past information, or the value of $m$.  

Based on the definition of conditional probability, we have:  

\begin{equation}
  \begin{aligned}
    P( X \geqslant m + k | X \geqslant m) =& \frac{P(\{ X \geqslant m + k  \} \cap \{ X \geqslant m \})}{P(\{ X \geqslant m \})}  \\
    \text{Since} \, m+k > m \, \text{:}& \, \{ X \geqslant m + k  \} \cap \{ X \geqslant m \} = \{ X \geqslant m + k  \}  \\
    \text{thus:}&  \\
    P( X \geqslant m + k | X \geqslant m) =& \frac{P(\{ X \geqslant m + k  \})}{P(\{ X \geqslant m \})}  \\
                                          =& \frac{1 - P(\{ X \geqslant m + k  \}^c)}{1- P(\{ X \geqslant m \}^c)}  \\
                                          =& \frac{1 - P(X < m + k)}{1- P(X < m)}  \\
    \text{We know the cdf of X is:}& \, F_X(x) = (1 - e^{-\lambda*x})*1_{[0,\infty)}(x)  \\
    \text{thus:}&  \\
    P( X \geqslant m + k | X \geqslant m) =& \frac{\left[1 - (1 - e^{-\lambda*(m+k)})\right]*1_{[0,\infty)}(m+k)}
                                                  {\left[1 - (1 - e^{-\lambda*m})\right]*1_{[0,\infty)}(m)}  \\
                                          =& \frac{1 - 1 + e^{-\lambda*(m+k)}}{1 - 1 + e^{-\lambda*m}}  \\
                                          =& \frac{e^{-\lambda*(m+k)}}{e^{-\lambda*m}}  \\
                                          =& e^{-\lambda*k}  \\
                                          =& 1 - 1 + e^{-\lambda*k}  \\
                                          =& 1 - (1 - e^{-\lambda*k})  \\
                                          =& 1 - P(X < k)  \\
                                          =& P(X \geqslant k) \quad \text{Q.E.D.}  \\
  \end{aligned}
\end{equation}  

# **Question 3**  

## Find the probability density function of $X$  

We can see that $X$ is a continuous random variable as its cdf takes on a infinite number of value on $[0, + \infty)$.  

We have the cdf $F_X(x) = \frac{x}{1+x}*1_{[0,+\infty)}$.  

By the Fundamental Theorem of Calculus, the pdf can be found by differentiating the cdf. Thus, we find the pdf as below:  

\begin{equation}
  \begin{aligned}
    f_X(x) =& \frac{d}{dx}[F_X(x)]  \\
           =& \left( \frac{x}{1+x} \right)' * 1_{[0,+\infty)}  \\
           =& \left[ \frac{1}{1+x} - \frac{1}{(1+x)^2}  \right] * 1_{[0,+\infty)}  \\
           =& \frac{1+x-1}{(1+x)^2} * 1_{[0,+\infty)}  \\
           =& \frac{x}{(1+x)^2} * 1_{[0,+\infty)}  \\
  \end{aligned}
\end{equation}  

## Calculate $P(2 < X < 3)$  

Because $X$ is a continuous random variable, it has zero point probability property, i.e. $P(X=x) = 0 \quad ,\forall x$.  

Thus, we have:  

\begin{equation}
  \begin{aligned}
    P(2 < X < 3) =& P(2 < X \leqslant 3)  \\
                 =& P(\{X \leqslant 3\} \backslash \{X \leqslant 2\})  \\
                 =& P(X \leqslant 3) - P(X \leqslant 2) \quad \text{(property of probability measure)}  \\
                 =& F_X(3) - F_X(2)  \\
                 =& \frac{3}{3+1} - \frac{2}{2+1}  \\
                 =& \frac{3}{4} - \frac{2}{3}  \\
                 =& \frac{1}{12}
  \end{aligned}
\end{equation}  

## Calculate $E\left( {(1+X)^2} {e^{-2X}} \right)$  

We have:  

\begin{equation}
  \begin{aligned}
    E\left[ {(1+X)^2}{e^{-2X}} \right] =& \int {{(1+u)^2}{e^{-2u}}} {f_X(u)}du  \\
                                       =& \int_{-\infty}^0 {{(1+u)^2} {e^{-2u}} {0} du} + 
                                          \int_0^{\infty} {{(1+u)^2} {e^{-2u}}} {\frac{u}{(1+u)^2}}du  \\
                                       =& 0 + \int_0^{\infty} {{(1+u)^2} {e^{-2u}}} {\frac{u}{(1+u)^2}}du  \\
                                       =& \int_0^{\infty} {u} {e^{-2u}} du  \\
  \end{aligned}
\end{equation}  

Integrating by parts with $\left\{ \begin{matrix} r = u \\ ds = e^{-2u} du \end{matrix}  \right.$  

We have $\left\{ \begin{matrix} dr = du \\ ds = \int {e^{-2u} du} = -\frac{1}{2} \int {e^{-2u}(-2)}du = -\frac{1}{2} \int {e^{-2u}d(-2u)} = -\frac{1}{2} e^{-2u} \end{matrix}  \right.$    

Thus:  

\begin{equation}
  \begin{aligned}
    E\left[ {(1+X)^2}{e^{-2X}} \right] =& -\frac{u}{2} \int_0^{\infty} {e^{-2u}} d(-2u) - 
                                          \int_0^{\infty} {{-\frac{1}{2}} {e^{-2u}}} du  \\
                                       =& -\frac{u}{2} \int_0^{\infty} {e^{-2u}} d(-2u) - 
                                          \frac{1}{4} \int_0^{\infty} {e^{-2u}} d(-2u)  \\
                                       =& \left( -\frac{u}{2} -\frac{1}{4} \right) {\int_0^{\infty} {e^{-2u}} d(-2u)}  \\ 
                                       =& \lim_{t \to \infty} \left( -\frac{2u+1}{4} * e^{-2u} \Big|_0^t \right)  \\
                                       =& \lim_{t \to \infty} \left( -\frac{2t+1}{4} * e^{-2t} + \frac{1}{4}  \right)  \\
                                       =& \lim_{t \to \infty} \left( -\frac{2t+1}{4} * e^{-2t} \right) + 
                                          \lim_{t \to \infty} \frac{1}{4}  \quad \text{(sum of limits rule)}  \\
                                       =& \left[\lim_{t \to \infty} \left( -\frac{2t+1}{4} \right) * 
                                          \lim_{t \to \infty} \left( e^{-2t} \right)\right] + 
                                          \frac{1}{4}  \quad \text{(product of limits rule)}  \\
                                       =& \left[\lim_{t \to \infty} \left( -\frac{2t+1}{4} \right)\right] * 0 + \frac{1}{4}  \\
                                       =& \frac{1}{4}  \\
  \end{aligned}
\end{equation}  
